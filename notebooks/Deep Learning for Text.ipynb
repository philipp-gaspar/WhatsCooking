{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# FOLDERS\n",
    "package_path = os.path.dirname(os.getcwd())\n",
    "data_path = package_path + '/data/'\n",
    "\n",
    "# LOAD DATA\n",
    "input_name = 'train.json'\n",
    "input_file = data_path + input_name\n",
    "\n",
    "df = pd.read_json(input_file)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ingredients` entries in the dataframe are lists and within each **list** entry has an individual ingredient of a particular recipe. To make the tokenization process easier we will need to convert each recipe into a **string** object, so let's create a new column for the dataframe called `ingredients_str`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    romaine lettuce, black olives, grape tomatoes,...\n",
       "1    plain flour, ground pepper, salt, tomatoes, gr...\n",
       "2    eggs, pepper, salt, mayonaise, cooking oil, gr...\n",
       "3                    water, vegetable oil, wheat, salt\n",
       "4    black pepper, shallots, cornflour, cayenne pep...\n",
       "Name: ingredients_str, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ingredients_str'] = [', '.join(i).strip() for i in df['ingredients']] \n",
    "\n",
    "df['ingredients_str'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each recipe is a **string** with the ingredients separated by commas. Now it should be easier to proceed with the tokenization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using *Keras* for word level representations.\n",
    "\n",
    "The `Keras` library has a class to deal with the tokenization of text documents. Next, we are going to use the `Tokenizer` class to create three types of **word level representations**:\n",
    "\n",
    "* One-Hot Encoding\n",
    "* Frequency Document\n",
    "* TF-IDF\n",
    "\n",
    "Note that for this problem the expression **word level representationg** does not mean that we re going to tokenize each word separately. What we are goint to do is to tokenize each ingredient inside a particular recipe, even if it has more than one word. The example bellow could be used to clarify.\n",
    "\n",
    "**Task:** Tokenize the following recipe: *ground black pepper, cold water*\n",
    "\n",
    "* ['ground', 'black', 'pepper', 'cold', 'water]\n",
    "\n",
    "But for this problem we will tokenize the recipe in the following way:\n",
    "\n",
    "* ['ground black pepper', 'cold water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a Tokenizer object using all the words in the vocabulary\n",
    "tokenizer = Tokenizer(num_words=None, split=', ', lower=True)\n",
    "\n",
    "# builds the word index\n",
    "tokenizer.fit_on_texts(df['ingredients'])\n",
    "\n",
    "# recover the word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# turn strings into lists of integer indices\n",
    "sequences = tokenizer.texts_to_sequences(df['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index[salt] = 1\n",
      "word_index[onions] = 2\n",
      "word_index[olive oil] = 3\n",
      "word_index[water] = 4\n",
      "word_index[garlic] = 5\n"
     ]
    }
   ],
   "source": [
    "for key in list(word_index.keys())[:5]:\n",
    "    print('word_index[%s] = %s' % (key, word_index[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the `word_index` is a dictionary with a integer index for each word/ingredient present in the dataset. The words/ingredients are automatically organized by the most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly get the representations\n",
    "one_hot_data = tokenizer.texts_to_matrix(df['ingredients'], mode='binary')\n",
    "freq_data = tokenizer.texts_to_matrix(df['ingredients'], mode='freq')\n",
    "tfidf_data = tokenizer.texts_to_matrix(df['ingredients'], mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset with the new representation have:\n",
      "  - 39774 entries/recipes\n",
      "  - 6715 features/ingredients\n"
     ]
    }
   ],
   "source": [
    "n_samples = tfidf_data.shape[0]\n",
    "n_features = tfidf_data.shape[1]\n",
    "\n",
    "print('The training dataset with the new representation have:')\n",
    "print('  - %i entries/recipes' % n_samples)\n",
    "print('  - %i features/ingredients' % n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cross Validation Partitions\n",
    "\n",
    "As we know from the EDA notebook of this dataset, the classes are unbalanced. So, we are going to use the class `StratifiedKFold` from the **Sklearn** library to help us to create folds which are made by preserving the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 20 unique classes.\n"
     ]
    }
   ],
   "source": [
    "# construct the target vector\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "# categorical target (one-hot encoded)\n",
    "lb = LabelBinarizer()\n",
    "target_cat = lb.fit_transform(df['cuisine']) \n",
    "\n",
    "# integer target, used in the StratifiedKfold class \n",
    "# in order to make each fold with balanced classes\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(df['cuisine']) \n",
    "\n",
    "n_classes = len(np.unique(target))\n",
    "print('The dataset has %i unique classes.' % n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1 has 80% events for training and 20% for validation.\n",
      "Fold #2 has 80% events for training and 20% for validation.\n",
      "Fold #3 has 80% events for training and 20% for validation.\n",
      "Fold #4 has 80% events for training and 20% for validation.\n",
      "Fold #5 has 80% events for training and 20% for validation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits = 5\n",
    "seed = 2018\n",
    "folds = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed).split(tfidf_data, target))\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    percentage_trn = (len(fold[0]) / n_samples) * 100\n",
    "    percentage_val = (len(fold[1]) / n_samples) * 100\n",
    "    print('Fold #%i has %1.0f%% events for training and %1.0f%% for validation.' % (i+1, percentage_trn, percentage_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the amount of events in training and validation sets atre the same for every fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model\n",
    "\n",
    "Now it's time to create a Deep Neural Network Model to train our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Function to create the Neural Network Model\n",
    "    \"\"\"\n",
    "    K.clear_session()\n",
    "    \n",
    "    # creating the Deep Neural Net Model\n",
    "    model = Sequential()\n",
    "\n",
    "    # layer 1\n",
    "    model.add(Dense(units=512, \n",
    "                    activation='relu', \n",
    "                    input_shape=(tfidf_data.shape[1], )))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # layer 2\n",
    "    model.add(Dense(units=128, \n",
    "                    activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(units=n_classes,\n",
    "                    activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=Adam(lr=0.005), \n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# add callbacks to the model\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "callbacks = [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Fold 1# <<\n",
      "  Training on 31812 examples.\n",
      "  Validating on 7962 examples.\n",
      "7962/7962 [==============================] - 1s 100us/step\n",
      "  This model has 0.77 validation accuraccy.\n",
      "\n",
      ">> Fold 2# <<\n",
      "  Training on 31816 examples.\n",
      "  Validating on 7958 examples.\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "cv_hist = []\n",
    "\n",
    "# train the model\n",
    "for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "    print('>> Fold %i# <<' % int(fold+1))\n",
    "    \n",
    "    # get training and validation data folds\n",
    "    X_trn = tfidf_data[trn_idx, :]\n",
    "    y_trn = target_cat[trn_idx, :]\n",
    "    X_val = tfidf_data[val_idx, :]\n",
    "    y_val = target_cat[val_idx, :]\n",
    "    \n",
    "    print('  Training on %i examples.' % X_trn.shape[0])\n",
    "    print('  Validating on %i examples.' % X_val.shape[0])\n",
    "    \n",
    "    model = load_model()\n",
    "    \n",
    "    hist = model.fit(X_trn, y_trn, \n",
    "                     validation_data=(X_val, y_val), \n",
    "                     batch_size=64, \n",
    "                     epochs=100, \n",
    "                     callbacks=callbacks,\n",
    "                     verbose=0)\n",
    "    \n",
    "    \n",
    "    scores = model.evaluate(X_val, y_val)\n",
    "    print('  This model has %1.2f validation accuraccy.\\n' % scores[1])\n",
    "    \n",
    "    cv_scores.append(scores)\n",
    "    cv_hist.append(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
